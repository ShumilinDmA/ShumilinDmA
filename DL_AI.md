### Deep learning Specialization
Deep learning specialization is created by DeepLearning.AI with Andrew Ng in the lector role.

This amazing specialization tells about neural networks from ever beginning till advanced tricks. The first assignment was about writing neural networks from scratch by using the only NumPy library in an objective-oriented programming style. Here was covered backpropagation, activation functions, vectorization, and fully connected networks. The rest specialization was dedicated to Neural Networks with TensorFlow library.

As the next steps, there was an explanation of what stands behind regularization, dropout, vanishing, and exploding gradients and corresponding gradient clipping and appropriate weight initialization. To improve the convergence of a neural net was introduced batch normalization, different optimization technics, was explained the concept of scheduling learning rate and early stopping. 

After optimization of the model, we should be able to estimate the quality of the solution. The next part was talking about validation technics, bias and variance tradeoff, estimating human-level performance. A well-known fact, that using a pre-trained neural network to our specific task gives a much better result than training it from scratch, and probably from scratch this result will never be achieved. For this fact was introduced the concept of transfer learning and multitask learning.

Convolution Neural Networks was the theme of the following part. There was explained some classical architectures like VGG, ResNet, Inception, etc. There was cool homework in Keras to classify images by using a pre-trained network. After that was explained object detection task and introduced YOLO network. As a practical assignment was task with detecting cars by using the YOLO model from darkweb framework. Another topic that was here is face recognition with a solution based on siamese networks.

The last part was dedicated to Recurrent Neural Networks. I did practical assignments about text classification, sentiment classification, debiasing word embeddings. Here also was introduced the concept of attention. One of the most interesting assignments here was about the task, which is called "Trigger Word Detection". It is analog of well-known mechanism like "Ok, Google" and etc.

This specialization gives basic knowledge about deep learning and the ability to dive deeper into this field through projects, articles. Practice gave some experience with the Tensorflow framework for optimization tasks, writing custom networks, or finetuning pretrained architectures!


<br/><br>
More links:
* [Yandex and MIPT | Machine Learning and Data Analysis Specialization](https://github.com/ShumilinDmA/ShumilinDmA/blob/main/MLandDA_specialization.md)
* [Moscow Institute of Physics and Technology | Deep Learning School](https://github.com/ShumilinDmA/ShumilinDmA/blob/main/DLS.md)
